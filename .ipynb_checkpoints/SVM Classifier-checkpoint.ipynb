{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_arr(df):\n",
    "    \n",
    "    vals = []\n",
    "    for _, row in df.iterrows():\n",
    "        vals.extend(row.tolist())\n",
    "    return np.array([x for x in vals if str(x) != 'nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(path):\n",
    "    \n",
    "    '''\n",
    "    Gets a list of subject IDs and the file suffix, given a path to the data files. \n",
    "    \n",
    "    Note: subject ID must be only 2 characters for this to work, and all data files\n",
    "    must have same suffix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to the data files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of subject IDs\n",
    "    str\n",
    "        the suffix to the filenames\n",
    "    '''\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    subjects = [f[:2] for f in files]\n",
    "    suffix = files[0][2:]\n",
    "        \n",
    "    subjects.sort()\n",
    "    \n",
    "    return subjects, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_labels(y_data):\n",
    "    \n",
    "    '''\n",
    "    Randomly selects half of the labels in the data to switch to the other class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_data: array-like\n",
    "        label data to scramble\n",
    "    '''\n",
    "    \n",
    "    classes = list(set(y_data))\n",
    "    classes.sort()\n",
    "    \n",
    "    y_data_copy = y_data.copy()\n",
    "    to_change = random.sample([i for i, x in enumerate(y_data) if x == classes[0]], k=len(y_data)//4)\n",
    "    to_change.extend(random.sample([i for i, x in enumerate(y_data) if x == classes[1]], k=len(y_data)//4))\n",
    "    \n",
    "    for index in to_change:\n",
    "        if y_data[index] == classes[0]:\n",
    "            y_data[index] = classes[1]\n",
    "        else:\n",
    "            y_data[index] = classes[0]\n",
    "    \n",
    "    # Makes sure labels are scrambled properly\n",
    "    num_diff = sum(i != j for i, j in zip(y_data, y_data_copy))  \n",
    "    if num_diff != len(y_data)//2:\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Training\n",
    "\n",
    "<a href=\"https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\">Guide to SVM Training</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_block_length(path, subjects, suffix, roi, conds):\n",
    "    \n",
    "    '''\n",
    "    Gets the minimum and maximum lengths of the blocks in the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        minimum block length\n",
    "    int\n",
    "        maxmimum block length\n",
    "    '''\n",
    "    \n",
    "    min_bl, max_bl = math.inf, 0\n",
    "    for subject in subjects:\n",
    "        \n",
    "        path_to_file = path + subject + suffix\n",
    "        mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "\n",
    "        for scan in range(len(mat[0])):\n",
    "            for cond in conds:\n",
    "                for block in range(len(mat[0][scan][0][cond][0])):\n",
    "        \n",
    "                    block_data = []\n",
    "                    for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                        block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                    \n",
    "                    min_bl = min(min_bl, len(block_data))\n",
    "                    max_bl = max(max_bl, len(block_data))\n",
    "                    \n",
    "    print(f\"Min block length: {min_bl}\")\n",
    "    print(f\"Max block length: {max_bl}\")\n",
    "\n",
    "    return min_bl, max_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_data_by_shortest(path, subject, suffix, roi, conds, block_length, use_abs_to_rank):\n",
    "    \n",
    "    '''\n",
    "    Extracts individual subject data from the .mat files. Rank-orders based on \n",
    "    shortest block in subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)\n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    use_abs_to_rank: boolean\n",
    "        whether to use greatest absolute voxel values to rank-order vectors; \n",
    "        otherwise, use most positive voxel values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of voxel data (x_data) separated by individual blocks and the corresponding \n",
    "    labels (y_data)\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    path_to_file = path + subject + suffix\n",
    "    mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "    \n",
    "    ranked_indices = None\n",
    "    \n",
    "    # Run through and find shortest subject block\n",
    "    shortest_block_length = math.inf\n",
    "    for scan in range(len(mat[0])):\n",
    "\n",
    "        for cond in conds:\n",
    "\n",
    "            blocks = [x for x in range(len(mat[0][scan][0][cond][0]))]\n",
    "\n",
    "            for block in blocks:\n",
    "                block_data = []\n",
    "                for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                    # Extract all voxel data from individual TRs\n",
    "                    block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                    \n",
    "                if len(block_data) < shortest_block_length:\n",
    "                    shortest_block_length = min(shortest_block_length, len(block_data))\n",
    "                    \n",
    "                    if use_abs_to_rank:\n",
    "                        ranked_indices = [i for i in (np.array([abs(n) for n in block_data])).argsort(kind='mergesort')[-block_length:]]\n",
    "                        ranked_indices = np.flip(ranked_indices)\n",
    "                    else:\n",
    "                        ranked_indices = [i for i in (-np.array(block_data)).argsort(kind='mergesort')[:block_length]]\n",
    "    \n",
    "                x_data.append(block_data)\n",
    "                y_data.append('untrained' if 'untrained' in mat[0][scan][1][cond][0] else 'trained')\n",
    "    \n",
    "    # Run through and rank-order based on shortest subject block            \n",
    "    for block_n in range(len(x_data)):\n",
    "        x_data[block_n] = [x_data[block_n][i] for i in ranked_indices]\n",
    "    \n",
    "    data = {'x': x_data, 'y': y_data}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_data(path, subject, suffix, roi, conds, block_length, rank_block, use_abs_to_rank):\n",
    "    \n",
    "    '''\n",
    "    Extracts individual subject data from the .mat files. Rank-orders based on\n",
    "    specific block number within subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)\n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    rank_block: int\n",
    "        the sequential number of the block upon which to rank-order all other blocks \n",
    "        within the subject\n",
    "    use_abs_to_rank: boolean\n",
    "        whether to use greatest absolute voxel values to rank-order vectors; \n",
    "        otherwise, use most positive voxel values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of voxel data (x_data) separated by individual blocks and the corresponding \n",
    "    labels (y_data)\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    path_to_file = path + subject + suffix\n",
    "    mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "    \n",
    "    ranked_indices = None\n",
    "    \n",
    "    block_count = 0\n",
    "    for scan in range(len(mat[0])):\n",
    "\n",
    "        for cond in conds:\n",
    "\n",
    "            blocks = [x for x in range(len(mat[0][scan][0][cond][0]))]\n",
    "\n",
    "            for block in blocks:\n",
    "                block_count += 1\n",
    "                block_data = []\n",
    "                for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                    # Extract all voxel data from individual TRs\n",
    "                    block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                \n",
    "                if block_count == rank_block:\n",
    "                    if use_abs_to_rank:    \n",
    "                        ranked_indices = [i for i in (np.array([abs(n) for n in block_data])).argsort(kind='mergesort')[-block_length:]]\n",
    "                        ranked_indices = np.flip(ranked_indices)\n",
    "                    else:\n",
    "                        ranked_indices = [i for i in (-np.array(block_data)).argsort(kind='mergesort')[:block_length]]\n",
    "    \n",
    "                x_data.append(block_data)\n",
    "                y_data.append('untrained' if 'untrained' in mat[0][scan][1][cond][0] else 'trained')\n",
    "    \n",
    "    # Run through and rank-order based on subject block  \n",
    "    for block_n in range(len(x_data)):\n",
    "        x_data[block_n] = [x_data[block_n][i] if i  < len(x_data[block_n]) else 0.0 for i in ranked_indices]\n",
    "                \n",
    "    data = {'x': x_data, 'y': y_data}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(subjects, path, suffix, roi, conds, block_length, rank_block, use_abs_to_rank):\n",
    "    \n",
    "    '''\n",
    "    Generates entire dataset from subject list, partitioned by subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects: list\n",
    "        a list of subject IDs to extract data from\n",
    "    path: str\n",
    "        the path to the data files\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)    \n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    rank_block: int\n",
    "        the sequential number of the block upon which to rank-order all other blocks \n",
    "        within the subject\n",
    "    use_abs_to_rank: boolean\n",
    "        whether to use greatest absolute voxel values to rank-order vectors; otherwise, use\n",
    "        most positive voxel values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        voxel data with subject key\n",
    "    dict\n",
    "        label data with subject key\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    \n",
    "    x_data_indices = []\n",
    "    y_data_by_subject = dict()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        \n",
    "        subject_data = extract_subject_data(path, subject, suffix, roi, conds, block_length, rank_block, use_abs_to_rank)\n",
    "        x_data_indices.append(len(x_data))\n",
    "        y_data_by_subject[subject] = subject_data['y']\n",
    "        \n",
    "        x_data.extend(subject_data['x'])\n",
    "    \n",
    "    # MinMaxScaler scales each feature to values between 0 and 1 among all x data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_standardized = scaler.fit_transform(x_data)\n",
    "    \n",
    "    # Sorts block data into respective subject\n",
    "    x_data_by_subject = dict()\n",
    "    for i in range(len(subjects)):\n",
    "        subject = subjects[i]\n",
    "        start_index = x_data_indices[i]\n",
    "        end_index = x_data_indices[i+1] if i+1 < len(x_data_indices) else len(x_data)\n",
    "        \n",
    "        x_data_by_subject[subject] = x_standardized[start_index:end_index]\n",
    "    \n",
    "    x_data_by_subject = {k: v for k, v in sorted(x_data_by_subject.items(), key=lambda item: len(item[1]))}\n",
    "    y_data_by_subject = {k: v for k, v in sorted(y_data_by_subject.items(), key=lambda item: len(item[1]))}\n",
    "    \n",
    "    return x_data_by_subject, y_data_by_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_run(x_train, y_train, x_test, y_test, kernels, gamma_range, C_range):\n",
    "    \n",
    "    '''\n",
    "    Gets best hyperparameters (kernel, C, and gamma values) that optimize SVM's predictions for given\n",
    "    x and y test dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train: array-like\n",
    "        dataset of block data used to train classifier\n",
    "    y_train: array-like\n",
    "        dataset of label data used to train classifier\n",
    "    x_test: array-like\n",
    "        testing dataset of block data used to optimize hyperparameters on\n",
    "    y_test: array-like\n",
    "        testing dataset of label data used to optimize hyperparameters on\n",
    "    kernels: list\n",
    "        kernels to test (recommended options are 'linear', 'rbf', and 'sigmoid')\n",
    "    gamma_range: dict\n",
    "        dict that specifies the range of values of gamma to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "    C_range: dict\n",
    "        dict that specifies the range of values of C to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        best combination of parameters found from grid search\n",
    "    float\n",
    "        best accuracy obtained from testing\n",
    "    '''\n",
    "    \n",
    "    gamma_vals = np.logspace(gamma_range['start'], gamma_range['stop'], gamma_range['num'], base=gamma_range['base'])\n",
    "    C_vals = np.logspace(C_range['start'], C_range['stop'], C_range['num'], base=C_range['base'])\n",
    "\n",
    "    param_grid = ParameterGrid({'kernel': kernels, 'gamma': gamma_vals, 'C': C_vals})\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    \n",
    "    # Tests each parameter combination to find best one for given testing data\n",
    "    for params in list(param_grid):\n",
    "        \n",
    "        svclassifier = SVC(kernel=params['kernel'], gamma=params['gamma'], C=params['C'], max_iter=-1)\n",
    "        svclassifier.fit(x_train, y_train)\n",
    "        \n",
    "        curr_acc = svclassifier.score(x_test, y_test)\n",
    "        \n",
    "        if curr_acc > best_acc:\n",
    "            best_acc = curr_acc\n",
    "            best_params = params\n",
    "            \n",
    "    return best_params, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x_data, y_data, inner_subjects, outer_subject, scramble):\n",
    "    \n",
    "    '''\n",
    "    Splits voxel and label data into appropriate testing and training data for nested\n",
    "    cross-validation with SVM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data: dict\n",
    "        voxel data with subject key\n",
    "    y_data: dict\n",
    "        label data with subject key\n",
    "    inner_subjects: list\n",
    "        list of subject IDs of the inner test subjects\n",
    "    outer_subject: str\n",
    "        the ID of the outer test subject\n",
    "    scramble: boolean, optional\n",
    "        whether or not to scramble the labels when training, \n",
    "        default is False\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        blocks of voxel data for training use\n",
    "    list\n",
    "        training labels for respective blocks\n",
    "    list\n",
    "        blocks of voxel data from inner test subject(s) for testing use\n",
    "    list \n",
    "        labels for inner test subject(s)\n",
    "    list\n",
    "        blocks of voxel data from outer test subject for testing use\n",
    "    list\n",
    "        labels for outer test subject    \n",
    "    '''\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test_inner = []\n",
    "    y_test_inner = []\n",
    "    \n",
    "    x_test_outer = []\n",
    "    y_test_outer = []\n",
    "    \n",
    "    for subject in x_data.keys():\n",
    "        if subject == outer_subject:\n",
    "            x_test_outer.extend(x_data[subject])\n",
    "            y_test_outer.extend(y_data[subject])\n",
    "        elif subject in inner_subjects:\n",
    "            x_test_inner.extend(x_data[subject])\n",
    "            y_test_inner.extend(y_data[subject])\n",
    "        else:\n",
    "            x_train.extend(x_data[subject])\n",
    "            if scramble:\n",
    "                y_scrambled = y_data[subject].copy()\n",
    "                scramble_labels(y_scrambled)\n",
    "                y_train.extend(y_scrambled)\n",
    "            else:\n",
    "                y_train.extend(y_data[subject])\n",
    "            \n",
    "    return x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_params, grid_params, num_inner=1, scramble=False, rank_block=1, use_abs_to_rank=False):\n",
    "    \n",
    "    '''\n",
    "    Trains and tests the classifier for accuracy using SVMs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_params: dict\n",
    "        path: str\n",
    "            the path to the data files\n",
    "        roi: int\n",
    "            0 for V1 data, 1 for MT data\n",
    "        conds: list\n",
    "            list of integers specifying the conditional datasets to extract\n",
    "            (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)   \n",
    "    grid_params: dict\n",
    "        kernels: list\n",
    "            kernels to test (recommended options are 'linear', 'rbf', and 'sigmoid')\n",
    "        gamma: dict\n",
    "            dict that specifies the range of values of gamma to test; should include start, stop to range,\n",
    "            num of values, and the exponential base\n",
    "        C: dict\n",
    "            dict that specifies the range of values of C to test; should include start, stop to range,\n",
    "            num of values, and the exponential base\n",
    "    num_inner: int\n",
    "        number of inner subjects to test classifier on,\n",
    "        default is 1\n",
    "    scramble: boolean, optional\n",
    "        whether or not to scramble the labels when training, \n",
    "        default is False\n",
    "    rank_block: int\n",
    "        the sequential number of the block upon which to rank-order all other blocks \n",
    "        within the subject,\n",
    "        default is 1\n",
    "    use_abs_to_rank: boolean\n",
    "        whether to use greatest absolute voxel values to rank-order vectors; otherwise, use\n",
    "        most positive voxel values,\n",
    "        default is False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        data of inner subject combination testing accuracy\n",
    "    DataFrame\n",
    "        data of outer subject testing accuracy\n",
    "    '''\n",
    "    \n",
    "    subjects, suffix = get_subjects(data_params['path'])\n",
    "    \n",
    "    bmin, bmax = get_min_max_block_length(data_params['path'], subjects, suffix, data_params['roi'], data_params['conds'])\n",
    "    block_length = bmin\n",
    "    x_data, y_data = generate_dataset(subjects, data_params['path'], suffix, data_params['roi'], data_params['conds'], block_length, rank_block, use_abs_to_rank)\n",
    "    \n",
    "    # Sets up DataFrames used to track inner and outer subject test accuracies\n",
    "    cols = []\n",
    "    for combo in itertools.combinations(range(len(subjects)), num_inner):\n",
    "        col = ''\n",
    "        for subject in combo:\n",
    "            col += '/' + subjects[subject]\n",
    "        cols.append(col[1:])\n",
    "    inner_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    outer_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    \n",
    "    for outer_subject in subjects:\n",
    "        \n",
    "        print(f\"Currently on outer subject #{subjects.index(outer_subject)+1}.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inner_subjects = [s for s in subjects if s != outer_subject]\n",
    "        for inner_test_subjects in itertools.combinations((inner_subjects), num_inner):\n",
    "            \n",
    "            inner_test_subjects = list(inner_test_subjects)\n",
    "            \n",
    "            col = ''\n",
    "            for subject in inner_test_subjects:\n",
    "                col += '/' + subject\n",
    "            col = col[1:]\n",
    "            print(f\"Currently on combination of {col}.\")    \n",
    "            \n",
    "            x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer = split_dataset(x_data, y_data, inner_test_subjects, outer_subject, scramble)\n",
    "            assert(len(set(y_train)) == 2)\n",
    "            assert(len(set(y_test_inner)) == 2)\n",
    "            assert(len(set(y_test_outer)) == 2)\n",
    "            \n",
    "            # Gets optimal params for training dataset from grid search\n",
    "            opt_params, inner_acc = get_optimal_run(x_train, y_train, x_test_inner, y_test_inner, grid_params['kernels'], grid_params['gamma'], grid_params['C']) \n",
    "            \n",
    "            if opt_params is not None:\n",
    "                # Trains model using optimal params for this set\n",
    "                svclassifier = SVC(kernel=opt_params['kernel'], gamma=opt_params['gamma'], C=opt_params['C'], max_iter=-1)\n",
    "                svclassifier.fit(x_train, y_train)\n",
    "\n",
    "                outer_acc = svclassifier.score(x_test_outer, y_test_outer)\n",
    "\n",
    "                # Logs inner and outer subject accuracy data in DataFrame\n",
    "                inner_acc_report.at[outer_subject, col] = inner_acc\n",
    "                outer_acc_report.at[outer_subject, col] = outer_acc\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        # Prints how long it took for last outer subject test\n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        minutes = exec_time // 60\n",
    "        seconds = exec_time % 60\n",
    "        print(f\"Last turn took {minutes} minutes and {seconds} seconds.\")\n",
    "    \n",
    "    clear_output()\n",
    "    return inner_acc_report, outer_acc_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank-Ordering Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'output/mt/large/post_cp/'\n",
    "path = r'scans/output/large&small/'\n",
    "roi = 0                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [0, 2]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "\n",
    "gamma_range = {'start': -15, 'stop': 3, 'num': 19, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 15, 'num': 19, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "data_params = {'path': path, 'roi': roi, 'conds': conds}\n",
    "grid_params = {'gamma': gamma_range, 'C': C_range, 'kernels': kernels}\n",
    "\n",
    "for rank_block in range(1, 13):\n",
    "    inner_accs, outer_accs = train(data_params, grid_params, rank_block=rank_block)\n",
    "    inner_accs.to_csv(output_path + f'raw/inner_accs{rank_block}.csv')\n",
    "    outer_accs.to_csv(output_path + f'raw/outer_accs{rank_block}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(output_path + 'raw/inner*.csv'):\n",
    "    dfs.append(pd.read_csv(file, index_col=0))\n",
    "    \n",
    "pd.concat(dfs).groupby(level=0).mean().to_csv(output_path + 'inner_accs_avg.csv')\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(output_path + 'raw/outer*.csv'):\n",
    "    dfs.append(pd.read_csv(file, index_col=0))\n",
    "    \n",
    "pd.concat(dfs).groupby(level=0).mean().to_csv(output_path + 'outer_accs_avg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests with Different Number of Inner Subjects Per Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "\n",
    "gamma_range = {'start': -13, 'stop': 1, 'num': 32, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 11, 'num': 32, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "data_params = {'path': path, 'roi': roi, 'conds': conds}\n",
    "grid_params = {'gamma': gamma_range, 'C': C_range, 'kernels': kernels}\n",
    "\n",
    "inner_accs, outer_accs = train(data_params, grid_params, num_inner=2)\n",
    "\n",
    "inner_accs.to_csv('output/inner_accs32_2inner.csv')\n",
    "outer_accs.to_csv('output/outer_accs32_2inner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation(data_params, grid_params, inner_dist, outer_dist, runs=30, history=True, rank_block=1, output_path=''):\n",
    "    \n",
    "    '''\n",
    "    Performs a specified number of runs where data labels are scrambled.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_params: dict\n",
    "        contains specifications for data processing (see train method for documentation)\n",
    "    grid_params: dict\n",
    "        contains values for grid search (see train method for documentation)\n",
    "    inner_dist: list\n",
    "        holds accuracy values for individual inner subject tests\n",
    "    outer_dist: list\n",
    "        holds accuracy values for individual outer subject tests\n",
    "    runs: int\n",
    "        number of runs to perform, default is 30\n",
    "    history: boolean\n",
    "        whether to track accuracy over runs and output permutation accuracy plot, \n",
    "        default is True\n",
    "    rank_block: int\n",
    "        the sequential number of the block upon which to rank-order all other blocks \n",
    "        within the subject, \n",
    "        default is 1\n",
    "    output_path: str\n",
    "        path to which files should be saved,\n",
    "        default is current directory\n",
    "    '''\n",
    "    \n",
    "    subjects, _ = get_subjects(data_params['path'])\n",
    "    mult = len(subjects) * (len(subjects) - 1)\n",
    "    \n",
    "    if history:\n",
    "        outer_sample_means = []\n",
    "        for i in range(len(outer_dist)//mult):\n",
    "            outer_sample_means.append(np.mean(outer_dist[i*mult:(i+1)*mult]))\n",
    "        \n",
    "        x = [i for i in range(1, len(outer_sample_means)+1)]\n",
    "        if len(outer_sample_means) > 0:\n",
    "            y = [outer_sample_means[0]]\n",
    "            for i in range(2, len(outer_sample_means)+1):\n",
    "                y.append(np.mean(outer_sample_means[:i]))\n",
    "        else:\n",
    "            y = []\n",
    "        \n",
    "    for n in range(runs):\n",
    "        \n",
    "        for rank_block in range(1, 13):\n",
    "            print(f'On run #{n+1} of {runs}, rank-ordering by block {rank_block}.')\n",
    "            inner_accs, outer_accs = train(data_params, grid_params, scramble=True, rank_block=rank_block)\n",
    "        \n",
    "            inner_dist.extend(df_to_arr(inner_accs).tolist())\n",
    "            outer_dist.extend(df_to_arr(outer_accs).tolist())\n",
    "        \n",
    "            outer_sample_means.append(np.mean(df_to_arr(outer_accs)))\n",
    "            np.save(output_path + 'outer_dist.npy', outer_dist)\n",
    "            np.save(output_path + 'inner_dist.npy', inner_dist)\n",
    "\n",
    "            if history:\n",
    "                y.append(np.mean(outer_sample_means))\n",
    "                x.append(len(y))\n",
    "\n",
    "                plt.plot(x, y)\n",
    "                plt.xlabel('Run')\n",
    "                plt.ylabel('Overall Mean Accuracy')\n",
    "                plt.title('Overall Outer Subject Accuracy')\n",
    "                plt.savefig(output_path + 'perm_hist.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'output/v1/large/'\n",
    "path = r'scans/output/large&small/'\n",
    "roi = 0                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [0, 2]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "\n",
    "gamma_range = {'start': -15, 'stop': 3, 'num': 19, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 15, 'num': 19, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "data_params = {'path': path, 'roi': roi, 'conds': conds}\n",
    "grid_params = {'gamma': gamma_range, 'C': C_range, 'kernels': kernels}\n",
    "\n",
    "inner_dist = []\n",
    "outer_dist = []\n",
    "permutation(data_params, grid_params, inner_dist, outer_dist, runs=5, output_path=output_path)\n",
    "\n",
    "np.save(output_path + 'outer_dist.npy', outer_dist)\n",
    "np.save(output_path + 'inner_dist.npy', inner_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
