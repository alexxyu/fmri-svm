{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Neural Network\n",
    "\n",
    "DNN Guides:\n",
    "<br><a href=\"https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\">Optimizers</a>\n",
    "<br><a href=\"https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\">DNN Layers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_int(labels):\n",
    "    classes = list(set(labels))\n",
    "    return np.array([0 if label == classes[1] else 1 for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(path):\n",
    "    \n",
    "    '''\n",
    "    Gets a list of subject IDs and the file suffix, given a path to the data files. \n",
    "    \n",
    "    Note: subject ID must be only 2 characters for this to work, and all data files\n",
    "    must have same suffix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to the data files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of subject IDs\n",
    "    str\n",
    "        the suffix to the filenames\n",
    "    '''\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    subjects = [f[:2] for f in files]\n",
    "    suffix = files[0][2:]\n",
    "        \n",
    "    subjects.sort()\n",
    "    \n",
    "    return subjects, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_labels(y_data):\n",
    "    \n",
    "    '''\n",
    "    Randomly selects half of the labels in the data to switch to the other class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_data: array-like\n",
    "        label data to scramble\n",
    "    '''\n",
    "    \n",
    "    classes = list(set(y_data))\n",
    "    classes.sort()\n",
    "    \n",
    "    y_data_copy = y_data.copy()\n",
    "    for index in np.nditer(np.random.choice(len(y_data), size=len(y_data)//2, replace=False)):\n",
    "        \n",
    "        if y_data[index] == classes[0]:\n",
    "            y_data[index] = classes[1]\n",
    "        else:\n",
    "            y_data[index] = classes[0]\n",
    "    \n",
    "    # Makes sure labels are scrambled properly\n",
    "    num_diff = sum(i != j for i, j in zip(y_data, y_data_copy))  \n",
    "    if num_diff != len(y_data)//2:\n",
    "        raise ValueError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_block_length(path, subjects, suffix, roi, conds):\n",
    "    \n",
    "    '''\n",
    "    Gets the minimum and maximum lengths of the blocks in the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        minimum block length\n",
    "    int\n",
    "        maxmimum block length\n",
    "    '''\n",
    "    \n",
    "    min_bl, max_bl = math.inf, 0\n",
    "    for subject in subjects:\n",
    "        \n",
    "        path_to_file = path + subject + suffix\n",
    "        mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "\n",
    "        for scan in range(len(mat[0])):\n",
    "            for cond in conds:\n",
    "                for block in range(len(mat[0][scan][0][cond][0])):\n",
    "        \n",
    "                    block_data = []\n",
    "                    for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                        block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                    \n",
    "                    min_bl = min(min_bl, len(block_data))\n",
    "                    max_bl = max(max_bl, len(block_data))\n",
    "                    \n",
    "    print(f\"Min block length: {min_bl}\")\n",
    "    print(f\"Max block length: {max_bl}\")\n",
    "\n",
    "    return min_bl, max_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_data(path, subject, suffix, roi, conds, block_length, rank_first):\n",
    "    \n",
    "    '''\n",
    "    Extracts individual subject data from the .mat files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)\n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    rank_first: boolean\n",
    "        whether to use first block in subject to order the rest of the blocks for that subject\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of voxel data (x_data) separated by individual blocks and the corresponding labels (y_data)\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    path_to_file = path + subject + suffix\n",
    "    mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "    \n",
    "    ranked_indices = None\n",
    "    \n",
    "    # Run through and find shortest subject block\n",
    "    if rank_first:\n",
    "        shortest_block_length = math.inf\n",
    "        for scan in range(len(mat[0])):\n",
    "\n",
    "            for cond in conds:\n",
    "\n",
    "                blocks = [x for x in range(len(mat[0][scan][0][cond][0]))]\n",
    "\n",
    "                for block in blocks:\n",
    "                    block_data = []\n",
    "                    for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                        # Extract all voxel data from individual TRs\n",
    "                        block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                    if len(block_data) < shortest_block_length:\n",
    "                        shortest_block_length = min(shortest_block_length, len(block_data))    \n",
    "                        ranked_indices = [i for i in (np.array(block_data)).argsort()[-block_length:]]\n",
    "                        ranked_indices = np.flip(ranked_indices)\n",
    "    \n",
    "    # Run through and rank-order based on shortest subject block            \n",
    "    for scan in range(len(mat[0])):\n",
    "            \n",
    "        for cond in conds:\n",
    "            \n",
    "            blocks = [x for x in range(len(mat[0][scan][0][cond][0]))]\n",
    "            \n",
    "            for block in blocks:\n",
    "                block_data = []\n",
    "                for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                    # Extract all voxel data from individual TRs\n",
    "                    block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())          \n",
    "                if rank_first:\n",
    "                    # Rank-orders a given subject's block based on the order of its first encountered block\n",
    "                    block_data = [block_data[i] for i in ranked_indices]\n",
    "                else:\n",
    "                    # Filters for most active voxels in each block\n",
    "                    block_data.sort()\n",
    "                    block_data = block_data[-block_length:]\n",
    "\n",
    "                x_data.append(block_data)\n",
    "                y_data.append(mat[0][scan][1][cond][0])\n",
    "    \n",
    "    data = {'x': x_data, 'y': y_data}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(subjects, path, suffix, roi, conds, block_length, rank_first):\n",
    "    \n",
    "    '''\n",
    "    Generates entire dataset from subject list, partitioned by subject.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects: list\n",
    "        a list of subject IDs to extract data from\n",
    "    path: str\n",
    "        the path to the data files\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)    \n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    rank_first: boolean\n",
    "        whether to use first block in subject to order the rest of the blocks for that subject\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        voxel data with subject key\n",
    "    dict\n",
    "        label data with subject key\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    \n",
    "    x_data_indices = []\n",
    "    y_data_by_subject = dict()\n",
    "    \n",
    "    for subject in subjects:\n",
    "        \n",
    "        subject_data = extract_subject_data(path, subject, suffix, roi, conds, block_length, rank_first)\n",
    "        x_data_indices.append(len(x_data))\n",
    "        y_data_by_subject[subject] = subject_data['y']\n",
    "        \n",
    "        x_data.extend(subject_data['x'])\n",
    "    \n",
    "    # MinMaxScaler scales each feature to values between 0 and 1 among all x data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_standardized = scaler.fit_transform(x_data)\n",
    "    \n",
    "    # Sorts block data into respective subject\n",
    "    x_data_by_subject = dict()\n",
    "    for i in range(len(subjects)):\n",
    "        subject = subjects[i]\n",
    "        start_index = x_data_indices[i]\n",
    "        end_index = x_data_indices[i+1] if i+1 < len(x_data_indices) else len(x_data)\n",
    "        \n",
    "        x_data_by_subject[subject] = x_standardized[start_index:end_index]\n",
    "    \n",
    "    return x_data_by_subject, y_data_by_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x_data, y_data, inner_subjects, outer_subject, scramble):\n",
    "    \n",
    "    '''\n",
    "    Splits voxel and label data into appropriate testing and training data for nested\n",
    "    cross-validation with SVM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data: dict\n",
    "        voxel data with subject key\n",
    "    y_data: dict\n",
    "        label data with subject key\n",
    "    inner_subjects: list\n",
    "        list of subject IDs of the inner test subjects\n",
    "    outer_subject: str\n",
    "        the ID of the outer test subject\n",
    "    scramble: boolean, optional\n",
    "        whether or not to scramble the labels when training, \n",
    "        default is False\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        blocks of voxel data for training use\n",
    "    list\n",
    "        training labels for respective blocks\n",
    "    list\n",
    "        blocks of voxel data from inner test subject(s) for testing use\n",
    "    list \n",
    "        labels for inner test subject(s)\n",
    "    list\n",
    "        blocks of voxel data from outer test subject for testing use\n",
    "    list\n",
    "        labels for outer test subject    \n",
    "    '''\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test_inner = []\n",
    "    y_test_inner = []\n",
    "    \n",
    "    x_test_outer = []\n",
    "    y_test_outer = []\n",
    "    \n",
    "    for subject in x_data.keys():\n",
    "        if subject == outer_subject:\n",
    "            x_test_outer.extend(x_data[subject])\n",
    "            y_test_outer.extend(y_data[subject])\n",
    "        elif subject in inner_subjects:\n",
    "            x_test_inner.extend(x_data[subject])\n",
    "            y_test_inner.extend(y_data[subject])\n",
    "        else:\n",
    "            x_train.extend(x_data[subject])\n",
    "            if scramble:\n",
    "                y_scrambled = y_data[subject].copy()\n",
    "                scramble_labels(y_scrambled)\n",
    "                y_train.extend(y_scrambled)\n",
    "            else:\n",
    "                y_train.extend(y_data[subject])\n",
    "            \n",
    "    return x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(data_params, epochs=20, layer_size=256, num_inner=1, scramble=False, rank_first=True, shuffle=False):\n",
    "    \n",
    "    '''\n",
    "    Trains and tests the classifier for accuracy using NNs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_params: dict\n",
    "        path: str\n",
    "            the path to the data files\n",
    "        roi: int\n",
    "            0 for V1 data, 1 for MT data\n",
    "        conds: list\n",
    "            list of integers specifying the conditional datasets to extract\n",
    "            (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)\n",
    "    epochs: int\n",
    "        number of iterations to train model on\n",
    "    layer_size: int\n",
    "        size of hidden layer\n",
    "    num_inner: int\n",
    "        number of inner subjects to test classifier on,\n",
    "        default is 1\n",
    "    scramble: boolean, optional\n",
    "        whether or not to scramble the labels when training, \n",
    "        default is False\n",
    "    rank_first: boolean\n",
    "        whether to use first block in subject to order the rest of the blocks for that subject,\n",
    "        default is True\n",
    "    shuffle: boolean\n",
    "        whether to randomize which block to use in rank-ordering, \n",
    "        default is False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        data of inner subject combination testing accuracy\n",
    "    DataFrame\n",
    "        data of outer subject testing accuracy\n",
    "    '''\n",
    "    \n",
    "    subjects, suffix = get_subjects(data_params['path'])\n",
    "    \n",
    "    cols = []\n",
    "    for combo in itertools.combinations(range(len(subjects)), num_inner):\n",
    "        col = ''\n",
    "        for subject in combo:\n",
    "            col += '/' + subjects[subject]\n",
    "        cols.append(col[1:])\n",
    "\n",
    "    outer_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    val_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    \n",
    "    bmin, bmax = get_min_max_block_length(data_params['path'], subjects, suffix, data_params['roi'], data_params['conds'])\n",
    "    block_length = bmin\n",
    "    x_data, y_data = generate_dataset(subjects, data_params['path'], suffix, data_params['roi'], data_params['conds'], block_length, rank_first)\n",
    "    \n",
    "    for outer_subject in subjects:\n",
    "        \n",
    "        print(f\"Currently on outer subject #{subjects.index(outer_subject)+1}.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inner_subjects = [s for s in subjects if s != outer_subject]\n",
    "        for inner_test_subjects in itertools.combinations((inner_subjects), num_inner):\n",
    "            \n",
    "            inner_test_subjects = list(inner_test_subjects)\n",
    "\n",
    "            col = ''\n",
    "            for subject in inner_test_subjects:\n",
    "                col += '/' + subject\n",
    "            col = col[1:]\n",
    "            print(f\"Currently on combination of {col}.\")    \n",
    "            \n",
    "            x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer = split_dataset(x_data, y_data, inner_test_subjects, outer_subject, scramble)\n",
    "            \n",
    "            y_train = labels_to_int(y_train)\n",
    "            y_test_inner = labels_to_int(y_test_inner)\n",
    "            y_test_outer = labels_to_int(y_test_outer)\n",
    "            \n",
    "            x_train = np.array(x_train)\n",
    "            x_test_inner = np.array(x_test_inner)\n",
    "            x_test_outer = np.array(x_test_outer)\n",
    "            \n",
    "            model = Sequential([\n",
    "                    layers.Dense(layer_size, input_shape=(block_length,), activation=\"relu\"),\n",
    "                    layers.Dense(1, activation=\"sigmoid\")\n",
    "            ])\n",
    "\n",
    "            optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "            model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "            model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test_inner, y_test_inner), verbose=0)\n",
    "            outer_loss, outer_acc = model.evaluate(x_test_outer, y_test_outer, verbose=0)\n",
    "            val_loss, val_acc = model.evaluate(x_test_inner, y_test_inner, verbose=0)\n",
    "                \n",
    "            # logs inner and outer subject accuracy data in dataframe\n",
    "            outer_acc_report.at[outer_subject, col] = outer_acc\n",
    "            val_acc_report.at[outer_subject, col] = val_acc\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        minutes = exec_time // 60\n",
    "        seconds = exec_time % 60\n",
    "        print(f\"Last turn took {minutes} minutes and {seconds} seconds.\")\n",
    "    \n",
    "    clear_output()\n",
    "    return outer_acc_report, val_acc_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Runs of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer accuracy mean: 0.5587606430053711\n",
      "Validation accuracy mean: 0.558226466178894\n"
     ]
    }
   ],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "data_params = {'path': r'scans/output/PRE/', 'roi': 1, 'conds': [1, 3]}\n",
    "\n",
    "'''\n",
    "outer_accs, val_accs = []\n",
    "for i in range(5):\n",
    "    print(f'On run {i+1}.')\n",
    "    outer_acc_report, val_accs = trainNN(data_params, classes, layer_size=256, scramble=False)\n",
    "    outer_accs.extend(df_to_arr(outer_acc_report))\n",
    "'''\n",
    "\n",
    "outer_accs, val_accs = trainNN(data_params, layer_size=256, scramble=False)\n",
    "\n",
    "print(f\"Outer accuracy mean: {np.mean(df_to_arr(outer_accs))}\")\n",
    "print(f\"Validation accuracy mean: {np.mean(df_to_arr(val_accs))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last turn took 0 minutes and 26.890951 seconds.\n",
      "Currently on outer subject #9.\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 271us/sample - loss: 0.7288 - accuracy: 0.5000\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 224us/sample - loss: 0.7058 - accuracy: 0.5000\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 486us/sample - loss: 0.7230 - accuracy: 0.4375\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 314us/sample - loss: 0.7092 - accuracy: 0.4375\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 364us/sample - loss: 0.6799 - accuracy: 0.7500\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 222us/sample - loss: 0.8044 - accuracy: 0.5000\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 991us/sample - loss: 0.7276 - accuracy: 0.5000\n",
      "16/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 243us/sample - loss: 0.6953 - accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "data_params = {'path': r'scans/output/PRE/', 'roi': 1, 'conds': [1, 3]}\n",
    "\n",
    "outer_accs_unscrambled = []\n",
    "for _ in range(20):\n",
    "    outer_acc_report, vals_accs = trainNN(data_params, scramble=False)\n",
    "    outer_accs_unscrambled.extend(df_to_arr(outer_acc_report))\n",
    "    np.save('output/nn_outer.npy', outer_accs_unscrambled)\n",
    "    \n",
    "outer_accs_scrambled = []\n",
    "for _ in range(20):\n",
    "    outer_acc_report, val_accs = trainNN(data_params, scramble=True)\n",
    "    outer_accs_scrambled.extend(df_to_arr(outer_acc_report))\n",
    "    np.save('output/nn_outer_s.npy', outer_accs_scrambled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
