{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Training Functions\n",
    "\n",
    "<a href=\"https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\">Guide to SVM Training</a>\n",
    "### To-do:\n",
    "<ul>\n",
    "    <li> Refine grid search, increase accuracy of outer loop subjects </li>\n",
    "    <li> Run scrambled label data (and fix the scramble method) </li>\n",
    "    <li> Make data generation process more efficient/less redundant </li>\n",
    "    <li> Shorten/simplify parameter list </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects(path, removeML = False):\n",
    "    \n",
    "    '''\n",
    "    Gets a list of subject IDs and the file suffix, given a path to the data files. \n",
    "    \n",
    "    Note: subject ID must be only 2 characters for this to work, and all data files\n",
    "    must have same suffix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to the data files\n",
    "    removeML: boolean, optional\n",
    "        specifies whether subject ML should be skipped, default is False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of subject IDs\n",
    "    str\n",
    "        the suffix to the filenames\n",
    "    '''\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    subjects = [f[:2] for f in files]\n",
    "    suffix = files[0][2:]\n",
    "    \n",
    "    if removeML:\n",
    "        subjects.remove('ML')\n",
    "        \n",
    "    subjects.sort()\n",
    "    \n",
    "    return subjects, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scramble_labels(y_data, classes):\n",
    "    \n",
    "    '''\n",
    "    Randomly selects half of the labels in the data to switch to the other class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_data: array-like\n",
    "        label data to scramble\n",
    "    classes: list\n",
    "        the two different classes of labels\n",
    "    '''\n",
    "    \n",
    "    for index in np.nditer(np.random.choice(len(y_data), size=len(y_data)//2, replace=False)):\n",
    "        \n",
    "        if y_data[index] == classes[0]:\n",
    "            y_data[index] = classes[1]\n",
    "        else:\n",
    "            y_data[index] = classes[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_data(path, subject, suffix, roi, conds, block_length, shuffle):\n",
    "    \n",
    "    '''\n",
    "    Extracts individual subject data from the .mat files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        directory to data files\n",
    "    subject: str\n",
    "        ID of subject to load data for\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)\n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of voxel data (x_data) separated by individual blocks and the corresponding labels (y_data)\n",
    "    '''\n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    path_to_file = path + subject + suffix\n",
    "    mat = scipy.io.loadmat(path_to_file)['roi_scanData'][0][roi]\n",
    "    \n",
    "    shuffled_indices = None\n",
    "    \n",
    "    for scan in range(len(mat[0])):\n",
    "            \n",
    "        for cond in conds:\n",
    "            \n",
    "            for block in range(len(mat[0][scan][0][cond][0])):\n",
    "\n",
    "                block_data = []\n",
    "                for tr in range(len(mat[0][scan][0][cond][0][block][0])):\n",
    "                    \n",
    "                    # Extract all voxel data from individual TRs\n",
    "                    block_data.extend(mat[0][scan][0][cond][0][block][0][tr][0][0][0].tolist())\n",
    "                    \n",
    "                # Filters for most active voxels in each block\n",
    "                block_data.sort()\n",
    "                block_data = block_data[-block_length:]\n",
    "                    \n",
    "                if shuffle:\n",
    "                    if shuffled_indices is None:\n",
    "                        shuffled_indices = [i for i in range(len(block_data))]\n",
    "                        random.shuffle(shuffled_indices)\n",
    "                    block_data = [block_data[i] for i in shuffled_indices]\n",
    "                \n",
    "                x_data.append(block_data)\n",
    "                y_data.append(mat[0][scan][1][cond][0])\n",
    "                \n",
    "    data = {'x': x_data, 'y': y_data}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates training and testing data.\n",
    "'''\n",
    "def generate_data(subjects, inner_test_subjects, outer_test_subject, path, suffix, roi, conds, block_length, shuffle):\n",
    "    \n",
    "    '''\n",
    "    Generates training and testing data, which is separated into training data, inside testing data,\n",
    "    and outside testind data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects: list\n",
    "        a list of subject IDs to extract data from\n",
    "    inner_test_subject: list\n",
    "        list of indices of the inner test subjects\n",
    "    outer_test_subject: int\n",
    "        the index of the outer test subject\n",
    "    path: str\n",
    "        the path to the data files\n",
    "    suffix: str\n",
    "        ending suffix of the data filename\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)    \n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        blocks of voxel data for training use\n",
    "    list\n",
    "        training labels\n",
    "    list\n",
    "        inner test subject blocks of voxel data for testing use\n",
    "    list \n",
    "        testing labels for inner test subject\n",
    "    list\n",
    "        outer test subject blocks of voxel data for testing use\n",
    "    list\n",
    "        testing labels for outer test subject\n",
    "    '''\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    x_test_inner = []\n",
    "    y_test_inner = []\n",
    "    \n",
    "    x_test_outer = []\n",
    "    y_test_outer = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "        \n",
    "        subject_data = extract_subject_data(path, subject, suffix, roi, conds, block_length, shuffle)\n",
    "        if subject == outer_test_subject:\n",
    "            x_test_outer.extend(subject_data['x'])\n",
    "            y_test_outer.extend(subject_data['y'])\n",
    "        elif subject in inner_test_subjects:\n",
    "            x_test_inner.extend(subject_data['x'])\n",
    "            y_test_inner.extend(subject_data['y'])\n",
    "        else:\n",
    "            x_train.extend(subject_data['x'])\n",
    "            y_train.extend(subject_data['y'])\n",
    "    \n",
    "    x_train_len = len(x_train)\n",
    "    x_test_outer_len = len(x_test_outer)\n",
    "    \n",
    "    data.extend(x_train)\n",
    "    data.extend(x_test_outer)\n",
    "    data.extend(x_test_inner)\n",
    "    \n",
    "    # MinMaxScaler scales each feature to values between 0 and 1 among all x data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    x_normalized = scaler.fit_transform(data)\n",
    "    x_train, x_test_outer, x_test_inner = x_normalized[:x_train_len], x_normalized[x_train_len:x_train_len+x_test_outer_len], x_normalized[x_train_len+x_test_outer_len:]\n",
    "\n",
    "    y_train = np.stack(y_train, axis=0)\n",
    "    \n",
    "    return x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_run(x_train, y_train, x_test, y_test, kernels, gamma_range, C_range):\n",
    "    \n",
    "    '''\n",
    "    Gets best hyperparameters (kernel, C, and gamma values) that optimize SVM's predictions for given\n",
    "    x and y test dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train: array-like\n",
    "        dataset of block data used to train classifier\n",
    "    y_train: array-like\n",
    "        dataset of label data used to train classifier\n",
    "    x_test: array-like\n",
    "        testing dataset of block data used to optimize hyperparameters on\n",
    "    y_test: array-like\n",
    "        testing dataset of label data used to optimize hyperparameters on\n",
    "    kernels: list\n",
    "        kernels to test (recommended options are 'linear', 'rbf', and 'sigmoid')\n",
    "    gamma_range: dict\n",
    "        dict that specifies the range of values of gamma to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "    C_range: dict\n",
    "        dict that specifies the range of values of C to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        best combination of parameters found from grid search\n",
    "    float\n",
    "        best accuracy obtained from testing\n",
    "    '''\n",
    "    \n",
    "    gamma_vals = np.logspace(gamma_range['start'], gamma_range['stop'], gamma_range['num'], base=gamma_range['base'])\n",
    "    C_vals = np.logspace(C_range['start'], C_range['stop'], C_range['num'], base=C_range['base'])\n",
    "\n",
    "    param_grid = ParameterGrid({'kernel': kernels, 'gamma': gamma_vals, 'C': C_vals})\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_params = None\n",
    "    for params in list(param_grid):\n",
    "        \n",
    "        svclassifier = SVC(kernel=params['kernel'], gamma=params['gamma'], C=params['C'], max_iter=-1)\n",
    "        svclassifier.fit(x_train, y_train)\n",
    "        \n",
    "        curr_acc = svclassifier.score(x_test, y_test)\n",
    "        \n",
    "        if curr_acc > best_acc:\n",
    "            best_acc = curr_acc\n",
    "            best_params = params\n",
    "            \n",
    "    return best_params, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path, roi, conds, block_length, kernels, gamma_range, C_range, num_inner=1, scramble=False, shuffle=False):\n",
    "    \n",
    "    '''\n",
    "    Trains and tests the classifier for accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        the path to the data files\n",
    "    roi: int\n",
    "        0 for V1 data, 1 for MT data\n",
    "    conds: list\n",
    "        list of integers specifying the conditional datasets to extract\n",
    "        (0 for trained_cp, 1 for trained_ip, 2 for untrained_cp, 3 for untrained_ip)    \n",
    "    block_length: int\n",
    "        the number of voxels to standardize every block in the dataset to\n",
    "    kernels: list\n",
    "        kernels to test (recommended options are 'linear', 'rbf', and 'sigmoid')\n",
    "    gamma_range: dict\n",
    "        dict that specifies the range of values of gamma to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "    C_range: dict\n",
    "        dict that specifies the range of values of C to test; should include start, stop to range,\n",
    "        num of values, and the exponential base\n",
    "    num_inner: int\n",
    "        number of inner subjects to test classifier on, default is 1\n",
    "    scramble: boolean, optional\n",
    "        whether or not to scramble the labels when training, default is False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        data of inner subject combination testing accuracy\n",
    "    DataFrame\n",
    "        data of outer subject testing accuracy\n",
    "    '''\n",
    "    \n",
    "    subjects, suffix = get_subjects(path)\n",
    "    \n",
    "    cols = []\n",
    "    for combo in itertools.combinations(range(len(subjects)), num_inner):\n",
    "        col = ''\n",
    "        for subject in combo:\n",
    "            col += '/' + subjects[subject]\n",
    "        cols.append(col[1:])\n",
    "\n",
    "    inner_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    outer_acc_report = pd.DataFrame(index=subjects, columns=cols)\n",
    "    \n",
    "    for outer_subject in subjects:\n",
    "        \n",
    "        print(\"Currently on outer subject #%i.\" % (subjects.index(outer_subject)+1))\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        inner_subjects = [s for s in subjects if s != outer_subject]\n",
    "        for inner_subject_test in itertools.combinations((inner_subjects), num_inner):\n",
    "            \n",
    "            inner_subject_test = list(inner_subject_test)\n",
    "\n",
    "            col = ''\n",
    "            for subject in inner_subject_test:\n",
    "                col += '/' + subject\n",
    "            col = col[1:]\n",
    "            print(\"Currently on combination of %s.\" % (col))    \n",
    "            \n",
    "            x_train, y_train, x_test_inner, y_test_inner, x_test_outer, y_test_outer = generate_data(subjects, inner_subject_test, outer_subject, path, suffix, roi, conds, block_length, shuffle)\n",
    "            #print('Training size: %i \\t Inner testing size: %i \\t Outer testing size: %i' % (len(x_train), len(x_test_inner), len(x_test_outer)))\n",
    "            \n",
    "            if scramble:\n",
    "                scramble_labels(y_train, classes)\n",
    "                \n",
    "            # gets optimal params for training dataset from grid search\n",
    "            params, inner_acc = get_optimal_run(x_train, y_train, x_test_inner, y_test_inner, kernels, gamma_range, C_range) \n",
    "\n",
    "            # train model using optimal params for this set\n",
    "            svclassifier = SVC(kernel=params['kernel'], gamma=params['gamma'], C=params['C'], max_iter=-1)\n",
    "            svclassifier.fit(x_train, y_train)\n",
    "            \n",
    "            print('Testing outer subject...')\n",
    "            outer_acc = svclassifier.score(x_test_outer, y_test_outer)\n",
    "            \n",
    "            # logs inner and outer subject accuracy data in dataframe\n",
    "            inner_acc_report.at[outer_subject, col] = inner_acc\n",
    "            outer_acc_report.at[outer_subject, col] = outer_acc\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        exec_time = end_time - start_time\n",
    "        minutes = exec_time // 60\n",
    "        seconds = exec_time % 60\n",
    "        print('Last turn took %i minutes and %f seconds.' % (minutes, seconds))\n",
    "    \n",
    "    clear_output()\n",
    "    return inner_acc_report, outer_acc_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier/Visualizing Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "gamma_range = {'start': -13, 'stop': 1, 'num': 32, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 11, 'num': 32, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "inner_accs, outer_accs = train(path, roi, conds, block_length, kernels, gamma_range, C_range, shuffle=True)\n",
    "\n",
    "inner_accs.to_csv('output/inner_accs32_rand2.csv')\n",
    "outer_accs.to_csv('output/outer_accs32_rand2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "gamma_range = {'start': -13, 'stop': 1, 'num': 64, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 11, 'num': 64, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "inner_accs, outer_accs = train(path, roi, conds, block_length, kernels, gamma_range, C_range, num_inner=3)\n",
    "\n",
    "inner_accs.to_csv('output/inner_accs64_3inner.csv')\n",
    "outer_accs.to_csv('output/outer_accs64_3inner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "gamma_range = {'start': -13, 'stop': 1, 'num': 64, 'base': 2.0}\n",
    "C_range = {'start': -3, 'stop': 11, 'num': 64, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "inner_accs, outer_accs = train(path, roi, conds, block_length, kernels, gamma_range, C_range)\n",
    "\n",
    "inner_accs.to_csv('output/inner_accs64.csv', sep='\\t')\n",
    "outer_accs.to_csv('output/outer_accs64.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'scans/output/PRE/'\n",
    "roi = 1                            # V1-roi: 0, MT-roi: 1\n",
    "conds = [1, 3]                     # trained_cp: 0, trained_ip: 1, untrained_cp: 2, untrained_ip: 3\n",
    "block_length = 624\n",
    "\n",
    "gamma_range = {'start': -15, 'stop': 5, 'num': 32, 'base': 2.0}\n",
    "C_range = {'start': -5, 'stop': 15, 'num': 32, 'base': 2.0}\n",
    "kernels = ['rbf', 'sigmoid']\n",
    "\n",
    "inner_accs, outer_accs = train(path, roi, conds, block_length, kernels, gamma_range, C_range)\n",
    "\n",
    "inner_accs.to_csv('output/inner_accs32_more.csv', sep='\\t')\n",
    "outer_accs.to_csv('output/outer_accs32_more.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
